{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9181612,"sourceType":"datasetVersion","datasetId":5549583}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-15T20:26:04.499067Z","iopub.execute_input":"2024-08-15T20:26:04.500122Z","iopub.status.idle":"2024-08-15T20:26:04.512862Z","shell.execute_reply.started":"2024-08-15T20:26:04.500076Z","shell.execute_reply":"2024-08-15T20:26:04.511939Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/csvbashcommands/formatted_command_dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import GPTNeoForCausalLM, GPT2Tokenizer, Trainer, TrainingArguments\nfrom datasets import load_dataset, Dataset\n\n# Load tokenizer and model\ntokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-125M')\nmodel = GPTNeoForCausalLM.from_pretrained('EleutherAI/gpt-neo-125M')\n\nmodel.config.use_cache = False\n\n# Load your dataset\ndataset = load_dataset('csv', data_files='/kaggle/input/csvbashcommands/formatted_command_dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2024-08-15T20:42:50.565388Z","iopub.execute_input":"2024-08-15T20:42:50.566089Z","iopub.status.idle":"2024-08-15T20:42:51.731676Z","shell.execute_reply.started":"2024-08-15T20:42:50.566055Z","shell.execute_reply":"2024-08-15T20:42:51.730631Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token\nmodel.config.pad_token_id = tokenizer.eos_token_id","metadata":{"execution":{"iopub.status.busy":"2024-08-15T20:42:53.271444Z","iopub.execute_input":"2024-08-15T20:42:53.271876Z","iopub.status.idle":"2024-08-15T20:42:53.279767Z","shell.execute_reply.started":"2024-08-15T20:42:53.271843Z","shell.execute_reply":"2024-08-15T20:42:53.278183Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def tokenize_function(examples):\n    tokenized = tokenizer(examples['description'], padding='max_length', truncation=True, max_length=128)\n    tokenized['labels'] = tokenized['input_ids'].copy()\n    return tokenized","metadata":{"execution":{"iopub.status.busy":"2024-08-15T20:42:54.681040Z","iopub.execute_input":"2024-08-15T20:42:54.681991Z","iopub.status.idle":"2024-08-15T20:42:54.688215Z","shell.execute_reply.started":"2024-08-15T20:42:54.681958Z","shell.execute_reply":"2024-08-15T20:42:54.687237Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset['train'].column_names)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T20:42:57.857325Z","iopub.execute_input":"2024-08-15T20:42:57.857758Z","iopub.status.idle":"2024-08-15T20:43:06.941107Z","shell.execute_reply.started":"2024-08-15T20:42:57.857725Z","shell.execute_reply":"2024-08-15T20:43:06.939862Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/481 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf812c848438429fb0e034f5dd40ad7b"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])","metadata":{"execution":{"iopub.status.busy":"2024-08-15T20:43:08.551537Z","iopub.execute_input":"2024-08-15T20:43:08.552285Z","iopub.status.idle":"2024-08-15T20:43:08.558707Z","shell.execute_reply.started":"2024-08-15T20:43:08.552252Z","shell.execute_reply":"2024-08-15T20:43:08.557683Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=500,\n    save_steps=1000,\n    load_best_model_at_end=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T20:43:10.691911Z","iopub.execute_input":"2024-08-15T20:43:10.692300Z","iopub.status.idle":"2024-08-15T20:43:10.734403Z","shell.execute_reply.started":"2024-08-15T20:43:10.692271Z","shell.execute_reply":"2024-08-15T20:43:10.733462Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        outputs = model(**inputs)\n        loss = outputs.loss\n        return (loss, outputs) if return_outputs else loss\n\n# Use CustomTrainer instead of Trainer\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset['train'],\n    eval_dataset=tokenized_dataset['test'] if 'test' in tokenized_dataset else None,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T20:43:12.897372Z","iopub.execute_input":"2024-08-15T20:43:12.898322Z","iopub.status.idle":"2024-08-15T20:43:13.069597Z","shell.execute_reply.started":"2024-08-15T20:43:12.898284Z","shell.execute_reply":"2024-08-15T20:43:13.068453Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T20:43:15.552169Z","iopub.execute_input":"2024-08-15T20:43:15.552690Z","iopub.status.idle":"2024-08-15T20:43:56.405749Z","shell.execute_reply.started":"2024-08-15T20:43:15.552648Z","shell.execute_reply":"2024-08-15T20:43:56.404771Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [93/93 00:39, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=93, training_loss=3.4743970645371305, metrics={'train_runtime': 40.2776, 'train_samples_per_second': 35.826, 'train_steps_per_second': 2.309, 'total_flos': 94230460366848.0, 'train_loss': 3.4743970645371305, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained(\"./fine_tuned_gpt_neo\")\ntokenizer.save_pretrained(\"./fine_tuned_gpt_neo\")","metadata":{"execution":{"iopub.status.busy":"2024-08-15T20:45:02.121474Z","iopub.execute_input":"2024-08-15T20:45:02.121914Z","iopub.status.idle":"2024-08-15T20:45:03.374061Z","shell.execute_reply.started":"2024-08-15T20:45:02.121882Z","shell.execute_reply":"2024-08-15T20:45:03.372773Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"('./fine_tuned_gpt_neo/tokenizer_config.json',\n './fine_tuned_gpt_neo/special_tokens_map.json',\n './fine_tuned_gpt_neo/vocab.json',\n './fine_tuned_gpt_neo/merges.txt',\n './fine_tuned_gpt_neo/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"model_path = \"./fine_tuned_gpt_neo\"\nmodel = GPTNeoForCausalLM.from_pretrained(model_path)\ntokenizer = GPT2Tokenizer.from_pretrained(model_path)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T20:54:31.173818Z","iopub.execute_input":"2024-08-15T20:54:31.174879Z","iopub.status.idle":"2024-08-15T20:54:31.812168Z","shell.execute_reply.started":"2024-08-15T20:54:31.174840Z","shell.execute_reply":"2024-08-15T20:54:31.810646Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T20:54:38.533326Z","iopub.execute_input":"2024-08-15T20:54:38.534513Z","iopub.status.idle":"2024-08-15T20:54:38.551121Z","shell.execute_reply.started":"2024-08-15T20:54:38.534468Z","shell.execute_reply":"2024-08-15T20:54:38.549980Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"GPTNeoForCausalLM(\n  (transformer): GPTNeoModel(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(2048, 768)\n    (drop): Dropout(p=0.0, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPTNeoBlock(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPTNeoAttention(\n          (attention): GPTNeoSelfAttention(\n            (attn_dropout): Dropout(p=0.0, inplace=False)\n            (resid_dropout): Dropout(p=0.0, inplace=False)\n            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPTNeoMLP(\n          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\ndef generate_bash_command(prompt, max_length=50):\n    full_prompt = f\"Human: {prompt}\\nBash command:\"\n    input_ids = tokenizer.encode(full_prompt, return_tensors=\"pt\")\n    with torch.no_grad():\n        output = model.generate(\n            input_ids,\n            max_length=max_length,\n            num_return_sequences=1,\n            no_repeat_ngram_size=2,\n            temperature=0.7,\n            pad_token_id=tokenizer.eos_token_id,\n            do_sample=True\n        )\n    response = tokenizer.decode(output[0], skip_special_tokens=True)\n    bash_command = response.split(\"Bash command:\")[-1].strip()\n    return bash_command","metadata":{"execution":{"iopub.status.busy":"2024-08-15T21:01:55.392995Z","iopub.execute_input":"2024-08-15T21:01:55.393399Z","iopub.status.idle":"2024-08-15T21:01:55.402257Z","shell.execute_reply.started":"2024-08-15T21:01:55.393365Z","shell.execute_reply":"2024-08-15T21:01:55.401147Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"print(\"Enter a natural language prompt to get a bash command. Type 'quit' to exit.\")\nwhile True:\n    user_input = input(\"\\nHuman: \")\n    if user_input.lower() == 'quit':\n        break\n    \n    bash_command = generate_bash_command(user_input)\n    print(f\"Bash command: {bash_command}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-15T21:01:59.515852Z","iopub.execute_input":"2024-08-15T21:01:59.516750Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Enter a natural language prompt to get a bash command. Type 'quit' to exit.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nHuman:  find me where 'text.txt' is on my computer\n"},{"name":"stdout","text":"Bash command: --find <filename>\n\nWhen I run the --with-find, I get no output:\n[1] -find -v <file\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nHuman:  open up the app vscode\n"},{"name":"stdout","text":"Bash command: bash -c 'export PATH /path/to/the/script/file_name'\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nHuman:  create a new folder on the desktop\n"},{"name":"stdout","text":"Bash command: chmod 127.0.2.1 /mnt\n","output_type":"stream"}]}]}